{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import spearmanr \n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.spatial.distance import cdist \n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from boruta import BorutaPy\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# write all imported libraries to requirements.txt\n",
    "# !pipreqs --force ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('./dataset.csv')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot nan values count for each column\n",
    "dataframe.isnull().sum().plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Count of NaN values for each column')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Count of NaN values')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe[[\n",
    "  \"Gender\",\n",
    "  \"With_Kids\",\n",
    "  \"Basket_Size\",\n",
    "  \"Attire\",\n",
    "  \"Wash_Item\",\n",
    "  \"shirt_type\",\n",
    "]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = df.columns\n",
    "df_oh = df[col_list]\n",
    "df = df.drop(col_list, 1)\n",
    "df_oh = pd.get_dummies(df_oh)\n",
    "df = pd.concat([df, df_oh], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i in range(0, 4000):\n",
    "    records.append([str(df.columns[j]) for j in range(0, 13) if df.values[i,j] == 1])\n",
    "    \n",
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_results = apriori(records)\n",
    "association_results = list(association_results)\n",
    "len(association_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "association_r_df = pd.DataFrame(columns=['Items', 'Support', 'Confidence', 'Lift'])\n",
    "\n",
    "for item in association_results:\n",
    "    cnt += 1\n",
    "    # first index of the inner list\n",
    "    # Contains base item and add item\n",
    "    pair = item[0]\n",
    "    items = [x for x in pair]\n",
    "    if len(items) < 2:\n",
    "        association_r_df = association_r_df.append({'Items':items[0], 'Support':item[1], 'Confidence':item[2][0][2], 'Lift':item[2][0][3]}, ignore_index=True)\n",
    "        print(\"(Rule \" + str(cnt) + \") \" + items[0])\n",
    "    else:\n",
    "        association_r_df = association_r_df.append({'Items':items[0] + \" -> \" + items[1], 'Support':item[1], 'Confidence':item[2][0][2], 'Lift':item[2][0][3]}, ignore_index=True)\n",
    "        print(\"(Rule \" + str(cnt) + \") \" + items[0] + \" -> \" + items[1])\n",
    "    \n",
    "    # second index of the inner list\n",
    "    print(\"Support: \" + str(round(item[1],3)))\n",
    "    \n",
    "    # third index of the list located at 0th\n",
    "    # of the third index of the inner list \n",
    "    \n",
    "    print(\"Confidence: \" + str(round(item[2][0][2],4)))\n",
    "    print(\"Lift: \" + str(round(item[2][0][3],4)))\n",
    "    print(\"=====================================\")\n",
    "    \n",
    "display(association_r_df)\n",
    "association_r_df.to_csv('association_rules.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = dataframe.copy()\n",
    "\n",
    "print(p_df.isnull().sum())\n",
    "len(p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_df = p_df.dropna(subset=['Race', 'Gender', 'latitude', 'longitude'], inplace=False)\n",
    "\n",
    "# p_df['Body_Size'].fillna('moderate', inplace=True)\n",
    "# p_df['Age_Range'].fillna(p_df['Age_Range'].mean(), inplace=True)\n",
    "# p_df[\"With_Kids\"].fillna(\"no\", inplace=True)\n",
    "# p_df['Kids_Category'].fillna('no_kids', inplace=True)\n",
    "# p_df['Basket_Size'].fillna('small', inplace=True)\n",
    "# p_df['TimeSpent_minutes'].fillna(p_df['TimeSpent_minutes'].mean(), inplace=True)\n",
    "# p_df['buyDrinks'].fillna(0.0, inplace=True)\n",
    "# p_df['TotalSpent_RM'].fillna(0.0, inplace=True)\n",
    "\n",
    "# imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "# imputed_data = imp.fit_transform(p_df)\n",
    "\n",
    "# imp_df = pd.DataFrame(imputed_data, columns = p_df.columns)\n",
    "\n",
    "# display(imp_df)\n",
    "# imp_df.isnull().sum()\n",
    "\n",
    "# # plot nan values count for each column\n",
    "# imp_df.isnull().sum().plot(kind='bar', figsize=(10, 5))\n",
    "# plt.title('Count of NaN values for each column')\n",
    "# plt.xlabel('Columns')\n",
    "# plt.ylabel('Count of NaN values')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_data = pd.read_csv('./weather.csv')\n",
    "# weather_data = weather_data.drop(['Unnamed: 0'], axis=1)\n",
    "# weather_data.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add weather data to the dataframe\n",
    "# weather_data['date'] = weather_data['date'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d').strftime('%d/%m/%Y'))\n",
    "\n",
    "# # add weather column to imp_df and fill with NaN\n",
    "# imp_df['weather'] = np.nan\n",
    "\n",
    "# display(imp_df)\n",
    "\n",
    "# hour_index = ['0', '3', '6', '9', '12', '15', '18', '21']\n",
    "\n",
    "# # loop imp_df\n",
    "# for i in range(len(imp_df)):\n",
    "#     # loop weather_data\n",
    "#     for j in range(len(weather_data)):\n",
    "#         # check if date is the same\n",
    "#         if imp_df['Date'][i] == weather_data['date'][j] :\n",
    "#           if int(datetime.datetime.strptime(imp_df['Time'][i], '%H:%M:%S').strftime('%H')) == str(weather_data['hour'][j]).replace('00', ''):\n",
    "#             imp_df['weather'][i] = weather_data['desc'][j]\n",
    "#           else:\n",
    "#             actual_hour = int(datetime.datetime.strptime(imp_df['Time'][i], '%H:%M:%S').strftime('%H'))\n",
    "#             for k in range(len(hour_index)):\n",
    "#               is_in_between = False\n",
    "#               if k == len(hour_index) - 1:\n",
    "#                 is_in_between = actual_hour > int(hour_index[k])\n",
    "#               else: \n",
    "#                 is_in_between = actual_hour > int(hour_index[k]) and actual_hour < int(hour_index[k+1])\n",
    "#               if is_in_between and k != len(hour_index) - 1:\n",
    "#                 lower_bound = hour_index[k]\n",
    "#                 upper_bound = hour_index[k+1]\n",
    "#                 lower_bound_offset = abs(actual_hour - int(lower_bound))\n",
    "#                 upper_bound_offset = abs(actual_hour - int(upper_bound))\n",
    "#                 if lower_bound_offset < upper_bound_offset:\n",
    "#                   imp_df['weather'][i] = weather_data['desc'][j]\n",
    "#                 else:\n",
    "#                   imp_df['weather'][i] = weather_data['desc'][j+1]\n",
    "#               else:\n",
    "#                 imp_df['weather'][i] = weather_data['desc'][j]\n",
    "\n",
    "# display(weather_data)\n",
    "# display(imp_df)\n",
    "\n",
    "# imp_df['Day'] = imp_df['Date'].apply(lambda x: datetime.datetime.strptime(x, '%d/%m/%Y').strftime('%A'))\n",
    "# imp_df['Month'] = imp_df['Date'].apply(lambda x: datetime.datetime.strptime(x, '%d/%m/%Y').strftime('%B'))\n",
    "# imp_df['Year'] = imp_df['Date'].apply(lambda x: datetime.datetime.strptime(x, '%d/%m/%Y').strftime('%Y'))\n",
    "# imp_df['Time'] = imp_df['Time'].apply(lambda x: x.replace(';', ':'))\n",
    "# display(imp_df)\n",
    "# imp_df['Time'] = imp_df['Time'].apply(lambda x: datetime.datetime.strptime(x, '%H:%M:%S').strftime('%H'))\n",
    "# imp_df['Time'] = imp_df['Time'].apply(lambda x: 'Morning' if int(x) in range(6,12) else 'Afternoon' if int(x) in range(12,18) else 'Evening' if int(x) in range(18,24) else 'Night')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "# imp_df.to_csv('dataset_w_weather.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add RWI (Relative Wealth Index) to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp_df = pd.read_csv('dataset_w_weather.csv')\n",
    "# rwi_dataset = pd.read_csv('./scraper/mys_relative_wealth_index.csv')\n",
    "\n",
    "# rwi_df = pd.DataFrame(rwi_dataset)\n",
    "# rwi_df = rwi_df.dropna(subset=['latitude', 'longitude', 'rwi'], inplace=False)\n",
    "# rwi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def haversine(lon1, lat1, lon2, lat2):\n",
    "#     \"\"\"\n",
    "#     Calculate the great circle distance in kilometers between two points \n",
    "#     on the earth (specified in decimal degrees)\n",
    "#     \"\"\"\n",
    "#     # convert decimal degrees to radians \n",
    "#     lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "#     # haversine formula \n",
    "#     dlon = lon2 - lon1 \n",
    "#     dlat = lat2 - lat1 \n",
    "#     a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "#     c = 2 * asin(sqrt(a)) \n",
    "#     r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "#     return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp_df['rwi'] = np.nan\n",
    " \n",
    "# for i in range(len(imp_df)):\n",
    "#   # calculate every distance \n",
    "#   distance = []\n",
    "#   for j in range(len(rwi_df)):\n",
    "#     distance.append(haversine(imp_df['longitude'][i], imp_df['latitude'][i], rwi_df['longitude'][j], rwi_df['latitude'][j]))\n",
    "  \n",
    "#   # get the index of the minimum distance\n",
    "#   min_index = distance.index(min(distance))\n",
    "#   imp_df['rwi'][i] = rwi_df['rwi'][min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to csv\n",
    "# imp_df.to_csv('dataset_w_weather&rwi.csv', index=False)\n",
    "\n",
    "# imp_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE ABOVE CODES ARE USED TO CLEAN AND MERGE EXTERNAL DATASET TO THE ORIGINAL DATASET, THE RESULTING DATASET IS SAVED TO dataset_w_weather&rwi&city.csv THUS THE CODES ARE ALL COMMENTED OUT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.read_csv('./dataset_w_weather&rwi&city.csv')\n",
    "imp_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in imp_df.columns:\n",
    "  print(col, imp_df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = imp_df.copy()\n",
    "print(len(ll))\n",
    "# find unique combinations of latitude and longitude\n",
    "ll['lat_long'] = ll['latitude'].astype(str) + '_' + ll['longitude'].astype(str)\n",
    "ll['lat_long'].unique()\n",
    "len(ll['lat_long'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop all columns\n",
    "print(\"For Washer NO: \")\n",
    "print('===================================== \\n')\n",
    "for col in imp_df.columns:\n",
    "    col_1 = imp_df.loc[1:100,[col]]\n",
    "    target = imp_df.loc[1:100,['Washer_No']]\n",
    "    spearmanr_coef, p_value = spearmanr(col_1, target)\n",
    "    print(\"Column: \", col)\n",
    "    print (\"coefficient=\", spearmanr_coef)\n",
    "    print('p-value=', p_value)\n",
    "    print('===================================== \\n')\n",
    "    \n",
    "print(\"For Dryer NO: \")\n",
    "print('===================================== \\n')\n",
    "for col in imp_df.columns:\n",
    "    col_1 = imp_df.loc[1:100,[col]]\n",
    "    target = imp_df.loc[1:100,['Dryer_No']]\n",
    "    spearmanr_coef, p_value = spearmanr(col_1, target)\n",
    "    print(\"Column: \", col)\n",
    "    print (\"coefficient=\", spearmanr_coef)\n",
    "    print('p-value=', p_value)\n",
    "    print('===================================== \\n')\n",
    "    \n",
    "print(\"For Total Spent: \")\n",
    "print('===================================== \\n')\n",
    "for col in imp_df.columns:\n",
    "    col_1 = imp_df.loc[1:100,[col]]\n",
    "    target = imp_df.loc[1:100,['TotalSpent_RM']]\n",
    "    spearmanr_coef, p_value = spearmanr(col_1, target)\n",
    "    print(\"Column: \", col)\n",
    "    print (\"coefficient=\", spearmanr_coef)\n",
    "    print('p-value=', p_value)\n",
    "    print('===================================== \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = imp_df.apply(LabelEncoder().fit_transform)\n",
    "sns.set(rc={'figure.figsize':(12, 12)})\n",
    "correlation_matrix = imp_df.iloc[:,:].corr().round(1)\n",
    "sns.heatmap(data=correlation_matrix, annot=True)\n",
    "plt.savefig('correlation_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df['weather'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(18,4))\n",
    "weather_dist_data = pd.read_csv('./dataset_w_weather&rwi&city.csv')\n",
    "sns.countplot(x='weather', data=weather_dist_data)\n",
    "plt.show()\n",
    "plt.savefig('weather_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relationship between weather and washer no\n",
    "plt.figure(figsize=(16,4))\n",
    "sns.countplot(x='weather', hue='Washer_No', data=imp_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relationship between weather and dryer no\n",
    "plt.figure(figsize=(16,4))\n",
    "sns.countplot(x='weather', hue='Dryer_No', data=imp_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relationship between weather and total spent\n",
    "plt.figure(figsize=(16,4))\n",
    "sns.countplot(x='weather', hue='TotalSpent_RM', data=imp_df)\n",
    "plt.show()\n",
    "plt.savefig('weather_totalspent.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of rwi \n",
    "plt.figure(figsize=(12,4))\n",
    "sns.distplot(imp_df['rwi'])\n",
    "plt.show()\n",
    "plt.savefig('rwi_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplot for rwi\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.boxplot(imp_df['rwi'])\n",
    "plt.show()\n",
    "plt.savefig('rwi_boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a box plot for every column\n",
    "boxplottingdf = imp_df.copy()\n",
    "boxplottingdf = boxplottingdf.apply(LabelEncoder().fit_transform)\n",
    "for col in boxplottingdf.columns:\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.boxplot(boxplottingdf[col])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relationship between rwi and total spent\n",
    "plt.figure(figsize=(20,4))\n",
    "\n",
    "sns.regplot(x='rwi', y='TotalSpent_RM', data=imp_df)\n",
    "plt.show()\n",
    "\n",
    "coordinate = plt.figure(figsize=(8,8))\n",
    "coordinate = plt.scatter(imp_df['longitude'],imp_df['latitude'])\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relationship between attire and total spent\n",
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,4))\n",
    "sns.countplot(x='Attire', hue='Washer_No', data=imp_df, ax=ax[0])\n",
    "sns.countplot(x='Attire', hue='Dryer_No', data=imp_df, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(18,10))\n",
    "sns.countplot(x='Time', hue='Washer_No', data=imp_df, ax=ax[0, 0], palette='Set1')\n",
    "sns.countplot(x='Time', hue='Dryer_No', data=imp_df, ax=ax[0, 1])\n",
    "sns.countplot(x='Time', hue='TotalSpent_RM', data=imp_df, ax=ax[1, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between time spent in minutes and total spent and buy drinks\n",
    "plt.figure(figsize=(10,10))\n",
    "fig, (ax1, ax2) = plt.subplots(2,2, figsize=(18,10))\n",
    "sns.regplot(x='TimeSpent_minutes', y='TotalSpent_RM', data=imp_df, ax=ax1[0])\n",
    "sns.regplot(x='TimeSpent_minutes', y='buyDrinks', data=imp_df, ax=ax1[1])\n",
    "tsm_w = imp_df.apply(LabelEncoder().fit_transform)\n",
    "sns.regplot(x='rwi', y='TotalSpent_RM', data=imp_df, ax=ax2[0])\n",
    "sns.regplot(x='rwi', y='buyDrinks', data=imp_df, ax=ax2[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "sns.countplot(hue='TimeSpent_minutes', x='Wash_Item', data=imp_df)\n",
    "plt.show()\n",
    "plt.savefig('washitem_timespent.png')\n",
    "\n",
    "average_time = imp_df.groupby('Wash_Item')['TimeSpent_minutes'].mean()\n",
    "#plot avarge time spent for each wash item\n",
    "plt.figure(figsize=(10,4))\n",
    "average_time.plot(kind='bar')\n",
    "plt.show()\n",
    "plt.savefig('average_time-by_washItem.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (8 , 4))\n",
    "sns.countplot(y = 'Gender' , data = imp_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (8 , 4))\n",
    "sns.countplot(y = 'Time' , data = imp_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (8 , 4))\n",
    "sns.countplot(y = 'Day' , data = imp_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (18 , 6))\n",
    "n = 0 \n",
    "for x in [ 'Age_Range', 'Washer_No', 'Dryer_No']:\n",
    "    n += 1\n",
    "    plt.subplot(1 , 3 , n)\n",
    "    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)\n",
    "    sns.distplot(dataframe[x] , bins = 20)\n",
    "    plt.title('Distplot of {}'.format(x))\n",
    "    plt.savefig( 'Distplot of {}'.format(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate = plt.figure(figsize=(8,8))\n",
    "coordinate = plt.scatter(imp_df['longitude'],imp_df['latitude'])\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "coordinate.figure.savefig('Coordinate.png')\n",
    "coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_washer = plt.figure(figsize=(14,6))\n",
    "age_washer = plt.scatter(imp_df['rwi'],imp_df['TotalSpent_RM'])\n",
    "plt.xlabel('rwi')\n",
    "plt.ylabel('total spent')\n",
    "age_washer.figure.savefig('rwi_spent.png')\n",
    "age_washer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_washer = plt.figure(figsize=(14,6))\n",
    "age_washer = plt.scatter(imp_df['rwi'],imp_df['Age_Range'])\n",
    "plt.xlabel('rwi')\n",
    "plt.ylabel('Age Range')\n",
    "age_washer.figure.savefig('rwi_age.png')\n",
    "age_washer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the time columns\n",
    "imp_df = pd.read_csv('./dataset_w_weather&rwi&city.csv')\n",
    "display(imp_df.head())\n",
    "\n",
    "imp_df['Time'] = pd.Categorical(imp_df['Time'], categories=['Morning', 'Afternoon', 'Evening', 'Night'], ordered=True)\n",
    "plt.figure(figsize=(10, 10))\n",
    "pd.crosstab(imp_df['Time'], imp_df['Race'] ).plot()\n",
    "plt.savefig('relationship-between-race-and-time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(10, 7))\n",
    "\n",
    "# Creating plot\n",
    "plt.boxplot(imp_df['Age_Range'])\n",
    "\n",
    "plt.ylabel('Age_Range')\n",
    "# show plot\n",
    "plt.show()\n",
    "plt.savefig('boxplot-age.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What types of customers will likely to choose Washer No.2 and Dryer No.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = imp_df.copy()\n",
    "display(sim)\n",
    "\n",
    "display(sim['Washer_No'].value_counts())\n",
    "display(sim['Dryer_No'].value_counts())\n",
    "\n",
    "f_1 = sim['Washer_No'] == 3\n",
    "f_2 = sim['Dryer_No'] == 10\n",
    "customers = sim[f_1 & f_2].iloc[:,2:16]\n",
    "\n",
    "for column in customers.columns:\n",
    "    print(column + ': ' + str(customers[column].max()))\n",
    "    \n",
    "with open('washer_dryer_pred.txt', 'w') as f:\n",
    "    f.write('Washer_No: 2 and Dryer_No: 3' + '\\n\\n')\n",
    "    for column in customers.columns:\n",
    "      f.write(column + ': ' + str(customers[column].max()) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df['TotalSpent_RM'].plot(kind=\"hist\", figsize=(10,8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression for Age Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.read_csv('./dataset_w_weather&rwi&city.csv')\n",
    "imp_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection (LR - Age Range) *using boruta and rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_fs_df = imp_df.copy()\n",
    "ar_fs_df = ar_fs_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "ar_x = ar_fs_df.drop(['Age_Range'], axis=1)\n",
    "ar_y = ar_fs_df['Age_Range']\n",
    "\n",
    "display(ar_x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boruta\n",
    "def ranking(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x,2), ranks)\n",
    "    return dict(zip(names, ranks))\n",
    "  \n",
    "ar_fs_rf = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\", max_depth=5)\n",
    "ar_fs = BorutaPy(ar_fs_rf, n_estimators=\"auto\", random_state=1)\n",
    "ar_fs.fit(ar_x.values, ar_y.values.ravel())\n",
    "ar_boruta_score = ranking(list(map(float, ar_fs.ranking_)), ar_x.columns, order=-1)\n",
    "ar_boruta_score = pd.DataFrame(list(ar_boruta_score.items()), columns=['Features', 'Score'])\n",
    "ar_boruta_score = ar_boruta_score.sort_values(\"Score\", ascending = False)\n",
    "\n",
    "print('---------Top 10----------')\n",
    "display(ar_boruta_score.head(10))\n",
    "\n",
    "print('---------Bottom 10----------')\n",
    "display(ar_boruta_score.tail(10))\n",
    "\n",
    "sns_boruta_plot = sns.catplot(x=\"Score\", y=\"Features\", data = ar_boruta_score[:10], kind = \"bar\", height=14, aspect=2, palette='coolwarm')\n",
    "plt.title(\"Age Range LR Boruta Top 10 Features\")\n",
    "sns_boruta_plot.figure.savefig('Age_Range_LR_Boruta.png')\n",
    "\n",
    "pd.DataFrame.to_csv(ar_boruta_score.head(10), 'Age_Range_LR_Boruta.csv')\n",
    "\n",
    "# RFE\n",
    "ar_fs_rf = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\", max_depth=5, n_estimators=100)\n",
    "ar_fs_rf.fit(ar_x, ar_y)\n",
    "ar_rfe = RFECV(ar_fs_rf, min_features_to_select = 1, cv = 3)\n",
    "ar_rfe.fit(ar_x, ar_y)\n",
    "\n",
    "ar_rfe_score = ranking(list(map(float, ar_rfe.ranking_)), ar_x.columns, order=-1)\n",
    "ar_rfe_score = pd.DataFrame(list(ar_rfe_score.items()), columns=['Features', 'Score'])\n",
    "ar_rfe_score = ar_rfe_score.sort_values(\"Score\", ascending = False)\n",
    "\n",
    "print('---------Top 10----------')\n",
    "display(ar_rfe_score.head(10))\n",
    "\n",
    "print('---------Bottom 10----------')\n",
    "display(ar_rfe_score.tail(10))\n",
    "pd.DataFrame.to_csv(ar_rfe_score.head(10), 'AgeRange_RFE_Top10.csv')\n",
    "\n",
    "sns_wi_rfe_plot = sns.catplot(x=\"Score\", y=\"Features\", data = ar_rfe_score[0:10], kind = \"bar\", height=14, aspect=1.9, palette='coolwarm')\n",
    "plt.title(\"Age Range RFE Top-10 Features\")\n",
    "sns_wi_rfe_plot.figure.savefig('AgeRange_RFE.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_lrX = ar_x[[\"rwi\", \"TotalSpent_RM\",\"Time\", \"city\", \"TimeSpent_minutes\"]]\n",
    "ar_lrY = ar_y\n",
    "\n",
    "ar_lr_X_train, ar_lr_X_test, ar_lr_y_train, ar_lr_y_test = train_test_split(ar_lrX, ar_lrY, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(ar_lr_X_train.shape)\n",
    "print(ar_lr_X_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ar_reg = linear_model.LinearRegression()\n",
    "ar_lr_model = ar_reg.fit(ar_lr_X_train,ar_lr_y_train)  \n",
    "\n",
    "print (\"coefficients : \",ar_reg.coef_) #Slope\n",
    "print (\"Intercept : \",ar_reg.intercept_)\n",
    "\n",
    "ar_lr_model.score(ar_lr_X_train,ar_lr_y_train)\n",
    "\n",
    "def get_regression_predictions(input_features,intercept,slope):\n",
    "    predicted_values = input_features*slope + intercept\n",
    "    return predicted_values\n",
    "\n",
    "y_pred = ar_reg.predict(ar_lr_X_test)\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_pred - ar_lr_y_test)))\n",
    "print(\"Mean sum of squares (MSE): %.2f\" % np.mean((y_pred - ar_lr_y_test)** 2))\n",
    "print(\"R2-score: %.2f\" % r2_score(y_pred , ar_lr_y_test) )\n",
    "\n",
    "# plot slope \n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(ar_lr_X_train, ar_lr_y_train, 'o', label='original data', color='black')\n",
    "plt.plot(ar_lr_X_train, ar_reg.coef_*ar_lr_X_train + ar_reg.intercept_, 'r', label='fitted line', color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "filename = 'AgeRange_LR.sav'\n",
    "pickle.dump(ar_reg, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_svr = SVR(kernel = 'rbf')\n",
    "\n",
    "ar_svr.fit(ar_lr_X_train, ar_lr_y_train)\n",
    "y_pred = ar_svr.predict(ar_lr_X_test)\n",
    "\n",
    "print(\"Mean Absolute Error: {:.3f}\".format(mean_absolute_error(ar_lr_y_test, y_pred)))\n",
    "print(\"Mean Squared Error: {:.3f}\".format(mean_squared_error(ar_lr_y_test, y_pred)))\n",
    "print(\"R2 Score: {:.3f}\".format(r2_score(ar_lr_y_test, y_pred)))\n",
    "print(\"Accuracy: {:.3f}\".format(ar_svr.score(ar_lr_X_test, ar_lr_y_test)))\n",
    "\n",
    "gammas = [0.1, 1, 10, 100]\n",
    "svr_mae = [] \n",
    "svr_acc = []\n",
    "scr_mse = []\n",
    "for gamma in gammas:\n",
    "    ar_svr = SVR(kernel = 'rbf', gamma = gamma)\n",
    "    ar_svr.fit(ar_lr_X_train, ar_lr_y_train)\n",
    "    svr_y_pred = ar_svr.predict(ar_lr_X_test)\n",
    "    svr_mae.append(mean_absolute_error(ar_lr_y_test, svr_y_pred))\n",
    "    svr_acc.append(ar_svr.score(ar_lr_X_test, ar_lr_y_test)) \n",
    "    scr_mse.append(mean_squared_error(ar_lr_y_test, svr_y_pred))\n",
    "    \n",
    "plt.plot(gammas, svr_mae)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('gamma vs MAE').figure.savefig('AgeRange_SVR_Gamma_vs_Mae.png') \n",
    "plt.show()\n",
    "\n",
    "plt.plot(gammas, svr_acc)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('gamma vs Accuracy').figure.savefig('AgeRange_SVR_Gamma_vs_Accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gammas, scr_mse)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('gamma vs MSE').figure.savefig('AgeRange_SVR_Gamma_vs_MSE.png')\n",
    "plt.show()\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'AgeRange_SVR.sav'\n",
    "pickle.dump(ar_svr, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desicion Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_dtrX = ar_x\n",
    "ar_dtrY = ar_y\n",
    "display(ar_dtrX.head())\n",
    "ar_dtr_X_train, ar_dtr_X_test, ar_dtr_y_train, ar_dtr_y_test = train_test_split(ar_dtrX, ar_dtrY, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(ar_dtr_X_train.shape)\n",
    "print(ar_dtr_X_test.shape)\n",
    "\n",
    "ar_dtr = DecisionTreeRegressor(random_state=0, max_depth=7)\n",
    "ar_dtr.fit(ar_dtr_X_train, ar_dtr_y_train)\n",
    "\n",
    "# score model\n",
    "score = ar_dtr.score(ar_dtr_X_test, ar_dtr_y_test)\n",
    "print(score)\n",
    "\n",
    "# make predictions\n",
    "expected = ar_dtr_y_test\n",
    "predicted = ar_dtr.predict(ar_dtr_X_test)\n",
    "\n",
    "print(\"R2-score: %.2f\" % r2_score(predicted , expected) )\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(predicted - expected)))\n",
    "print(\"Mean sum of squares (MSE): %.2f\" % np.mean((predicted - expected) ** 2))\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'AgeRange_DTR.sav'\n",
    "pickle.dump(ar_dtr, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor (Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 30\n",
    "max_depth = [i for i in range(1,depth+1)] \n",
    "dtr_mae = [] \n",
    "for i in range(1,depth+1):\n",
    "    dtr = DecisionTreeRegressor(max_depth=i)\n",
    "    dtr.fit(ar_dtr_X_train, ar_dtr_y_train)\n",
    "    dtr_y_pred = dtr.predict(ar_dtr_X_test)\n",
    "    dtr_mae.append(mean_absolute_error(ar_dtr_y_test, dtr_y_pred))\n",
    "    \n",
    "plt.plot(max_depth, dtr_mae)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('max_depth vs MAE').figure.savefig('ar_dtr_depth_vs_mae.png') \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression for Number Of Baskets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.read_csv('./dataset_w_weather&rwi&city.csv')\n",
    "imp_df.head(5)\n",
    "\n",
    "nob_fs_df = imp_df.copy()\n",
    "nob_fs_df = ar_fs_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "nob_x = nob_fs_df.drop(['Num_of_Baskets'], axis=1)\n",
    "nob_y = nob_fs_df['Num_of_Baskets']\n",
    "\n",
    "display(ar_x.head())\n",
    "\n",
    "# Boruta\n",
    "def ranking(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x,2), ranks)\n",
    "    return dict(zip(names, ranks))\n",
    "\n",
    "nob_fs_rf = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\", max_depth=5)\n",
    "nob_fs = BorutaPy(nob_fs_rf, n_estimators=\"auto\", random_state=1)\n",
    "nob_fs.fit(nob_x.values, nob_y.values.ravel())\n",
    "nob_boruta_score = ranking(list(map(float, nob_fs.ranking_)), nob_x.columns, order=-1)\n",
    "nob_boruta_score = pd.DataFrame(list(nob_boruta_score.items()), columns=['Features', 'Score'])\n",
    "nob_boruta_score = nob_boruta_score.sort_values(\"Score\", ascending = False)\n",
    "\n",
    "print('---------Top 10----------')\n",
    "display(nob_boruta_score.head(10))\n",
    "\n",
    "print('---------Bottom 10----------')\n",
    "display(nob_boruta_score.tail(10))\n",
    "pd.DataFrame.to_csv(nob_boruta_score.head(10), 'NumOfBaskets_Boruta_Top10.csv')\n",
    "\n",
    "sns_boruta_plot = sns.catplot(x=\"Score\", y=\"Features\", data = nob_boruta_score[:10], kind = \"bar\", height=14, aspect=2, palette='coolwarm')\n",
    "plt.title(\"Num of Baskets LR Boruta Top 10 Features\")\n",
    "sns_boruta_plot.figure.savefig('NOB_LR_Boruta.png')\n",
    "\n",
    "pd.DataFrame.to_csv(nob_boruta_score.head(10), 'NOB_LR_Boruta.csv')\n",
    "\n",
    "# RFE\n",
    "nob_fs_rf = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\", max_depth=5, n_estimators=100)\n",
    "nob_fs_rf.fit(nob_x, nob_y)\n",
    "nob_rfe = RFECV(nob_fs_rf, min_features_to_select = 1, cv = 3)\n",
    "nob_rfe.fit(nob_x, nob_y)\n",
    "\n",
    "nob_rfe_score = ranking(list(map(float, nob_rfe.ranking_)), nob_x.columns, order=-1)\n",
    "nob_rfe_score = pd.DataFrame(list(nob_rfe_score.items()), columns=['Features', 'Score'])\n",
    "nob_rfe_score = nob_rfe_score.sort_values(\"Score\", ascending = False)\n",
    "\n",
    "print('---------Top 10----------')\n",
    "display(nob_rfe_score.head(10))\n",
    "\n",
    "print('---------Bottom 10----------')\n",
    "display(nob_rfe_score.tail(10))\n",
    "pd.DataFrame.to_csv(nob_rfe_score.head(10), 'NumOfBaskets_RFE_Top10.csv')\n",
    "\n",
    "sns_wi_rfe_plot = sns.catplot(x=\"Score\", y=\"Features\", data = nob_rfe_score[0:10], kind = \"bar\", height=14, aspect=1.9, palette='coolwarm')\n",
    "plt.title(\"Num of Baskets RFE Top-10 Features\")\n",
    "sns_wi_rfe_plot.figure.savefig('NOB_RFE.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nob_lrX = nob_x\n",
    "nob_lrY = nob_y\n",
    "\n",
    "nob_lr_X_train, nob_lr_X_test, nob_lr_y_train, nob_lr_y_test = train_test_split(nob_lrX, nob_lrY, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(nob_lr_X_train.shape)\n",
    "print(nob_lr_X_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nob_reg = linear_model.LinearRegression()\n",
    "nob_lr_model = nob_reg.fit(nob_lr_X_train,nob_lr_y_train)  \n",
    "\n",
    "print (\"coefficients : \",nob_reg.coef_) #Slope\n",
    "print (\"Intercept : \",nob_reg.intercept_)\n",
    "\n",
    "nob_lr_model.score(nob_lr_X_train,nob_lr_y_train)\n",
    "\n",
    "def get_regression_predictions(input_features,intercept,slope):\n",
    "    predicted_values = input_features*slope + intercept\n",
    "    return predicted_values\n",
    "\n",
    "y_pred = nob_reg.predict(nob_lr_X_test)\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_pred - nob_lr_y_test)))\n",
    "print(\"Mean sum of squares (MSE): %.2f\" % np.mean((y_pred - nob_lr_y_test)** 2))\n",
    "print(\"R2-score: %.2f\" % r2_score(y_pred , nob_lr_y_test) )\n",
    "\n",
    "# plot slope \n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(nob_lr_X_train, nob_lr_y_train, 'o', label='original data', color='black')\n",
    "plt.plot(nob_lr_X_train, nob_reg.coef_*nob_lr_X_train + nob_reg.intercept_, 'r', label='fitted line', color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    " \n",
    "# save the model to disk\n",
    "filename = 'nob_lr_model.sav'\n",
    "pickle.dump(nob_lr_model, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nob_lrX = nob_x[[\"TimeSpent_minutes\", \"rwi\", \"Age_Range\", \"weather\", \"TotalSpent_RM\", \"Shirt_Colour\", \"Basket_colour\"]]\n",
    "nob_lrY = nob_y\n",
    "\n",
    "nob_lrX = pd.get_dummies(nob_lrX, columns=[ \"weather\", \"Shirt_Colour\", \"Basket_colour\"])\n",
    "\n",
    "nob_dtr_X_train, nob_dtr_X_test, nob_dtr_y_train, nob_dtr_y_test = train_test_split(nob_lrX, nob_lrY, test_size=0.2, random_state=1)\n",
    "\n",
    "nob_dtr_reg = DecisionTreeRegressor(random_state=0, max_depth=100)\n",
    "\n",
    "# fit regressor with bX and Y data\n",
    "nob_dtr_reg.fit(nob_dtr_X_train, nob_dtr_y_train)\n",
    "\n",
    "# score model\n",
    "score = nob_dtr_reg.score(nob_dtr_X_test, nob_dtr_y_test)\n",
    "print(score)\n",
    "\n",
    "# make predictions\n",
    "expected = nob_dtr_y_test\n",
    "predicted = nob_dtr_reg.predict(nob_dtr_X_test)\n",
    "\n",
    "print(\"R2-score: %.2f\" % r2_score(predicted , expected) )\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(predicted - expected)))\n",
    "print(\"Mean sum of squares (MSE): %.2f\" % np.mean((predicted - expected) ** 2))\n",
    " \n",
    "# save the model to disk\n",
    "filename = 'nob_dtr_model.sav'\n",
    "pickle.dump(nob_dtr_reg, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression for Total Spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.read_csv('./dataset_w_weather&rwi&city.csv')\n",
    "imp_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x,2), ranks)\n",
    "    return dict(zip(names, ranks))\n",
    "  \n",
    "ts_fs_df = imp_df.copy()\n",
    "ts_fs_df = ts_fs_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "ts_X = ts_fs_df.drop(['TotalSpent_RM'], axis=1)\n",
    "ts_y = ts_fs_df['TotalSpent_RM']\n",
    "ts_rf = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\", max_depth=5)\n",
    "feat_selector = BorutaPy(ts_rf, n_estimators=\"auto\", random_state=1)\n",
    "feat_selector.fit(ts_X.values, ts_y.values.ravel())\n",
    "boruta_score = ranking(list(map(float, feat_selector.ranking_)), ts_X.columns, order=-1)\n",
    "boruta_score = pd.DataFrame(list(boruta_score.items()), columns=['Features', 'Score'])\n",
    "boruta_score = boruta_score.sort_values(\"Score\", ascending = False)\n",
    "\n",
    "print('---------Top 10----------')\n",
    "display(boruta_score.head(10))\n",
    "\n",
    "print('---------Bottom 10----------')\n",
    "display(boruta_score.tail(10))\n",
    "\n",
    "sns_boruta_plot = sns.catplot(x=\"Score\", y=\"Features\", data = boruta_score[:10], kind = \"bar\", height=14, aspect=2, palette='coolwarm')\n",
    "plt.title(\"Total Spent Boruta Top 10 Features\")\n",
    "sns_boruta_plot.figure.savefig('TotalSpent_Boruta.png')\n",
    "\n",
    "pd.DataFrame.to_csv(boruta_score.head(10), 'TotalSpent_Boruta_Top10.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR for total spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "ts_svr_X_train, ts_svr_X_test, ts_svr_y_train, ts_svr_y_test = train_test_split(ts_X, ts_y, test_size=0.2, random_state=1)\n",
    "\n",
    "ts_reg = SVR(kernel = 'rbf')\n",
    "\n",
    "ts_reg.fit(ts_svr_X_train, ts_svr_y_train)\n",
    "y_pred = ts_reg.predict(ts_svr_X_test)\n",
    "\n",
    "print(\"Mean Absolute Error: {:.3f}\".format(mean_absolute_error(ts_svr_y_test, y_pred)))\n",
    "print(\"Mean Squared Error: {:.3f}\".format(mean_squared_error(ts_svr_y_test, y_pred)))\n",
    "print(\"R2-score: {:.3f}\".format(r2_score(ts_svr_y_test, y_pred) ))\n",
    "print(\"accuracy_score: {:.3f}\".format(ts_reg.score(ts_svr_X_test, ts_svr_y_test)))\n",
    "\n",
    "gammas = [0.1, 1, 10, 100]\n",
    "svr_mae = [] \n",
    "svr_acc = []\n",
    "scr_mse = []\n",
    "for gamma in gammas:\n",
    "    ts_reg = SVR(kernel = 'rbf', gamma = gamma)\n",
    "    ts_reg.fit(ts_svr_X_train, ts_svr_y_train)\n",
    "    svr_y_pred = ts_reg.predict(ts_svr_X_test)\n",
    "    svr_mae.append(mean_absolute_error(ts_svr_y_test, svr_y_pred))\n",
    "    svr_acc.append(ts_reg.score(ts_svr_X_test, ts_svr_y_test)) \n",
    "    scr_mse.append(mean_squared_error(ts_svr_y_test, svr_y_pred))\n",
    "    \n",
    "plt.plot(gammas, svr_mae)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('gamma vs MAE').figure.savefig('TotalSpent_SVR_Gamma_vs_Mae.png') \n",
    "plt.show()\n",
    "\n",
    "plt.plot(gammas, svr_acc)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('gamma vs Accuracy').figure.savefig('TotalSpent_SVR_Gamma_vs_Accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gammas, scr_mse)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('gamma vs MSE').figure.savefig('TotalSpent_SVR_Gamma_vs_MSE.png')\n",
    "plt.show()\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'ts_svr_model.sav'\n",
    "pickle.dump(ts_reg, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression for Customer Numbers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.read_csv('./dataset_w_weather&rwi&city.csv')\n",
    "imp_df.head(5)\n",
    "\n",
    "numcust = imp_df.groupby(['Day', 'Time', 'weather']).size().reset_index(name='numcust')\n",
    "print(len(numcust))\n",
    "numcust.head(5)\n",
    "cn_df = numcust\n",
    "display(cn_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor for cust num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rtX = cn_df.drop(['numcust'], axis=1)\n",
    "# one hot encoding\n",
    "rtX = pd.get_dummies(rtX, columns=[\"Day\",\"Time\", \"weather\"])\n",
    "rtY = cn_df[['numcust']]\n",
    "\n",
    "rtx_train, rtx_test, rty_train, rty_test = train_test_split(rtX, rtY, test_size=0.2, random_state=1)\n",
    "\n",
    "print(rtx_train.shape)\n",
    "print(rtx_test.shape)\n",
    "\n",
    "# create decision tree regressor object\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "# fit regressor with bX and Y data\n",
    "regressor.fit(rtx_train, rty_train)\n",
    "\n",
    "# score model\n",
    "score = regressor.score(rtx_test, rty_test)\n",
    "print(score)\n",
    "\n",
    "# make predictions\n",
    "expected = rty_test\n",
    "predicted = regressor.predict(rtx_test)\n",
    "\n",
    "print(\"R2-score: %.2f\" % r2_score(predicted , expected) )\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(expected, predicted))\n",
    "print('RMSE: %.3f' % rmse)\n",
    "# calculate MAE\n",
    "mae = mean_absolute_error(expected, predicted)\n",
    "print('MAE: %.3f' % mae)\n",
    "  \n",
    "\n",
    "# plot rmse and mae\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(expected, 'o', color='green')\n",
    "plt.plot(predicted, 'o', color='orange')\n",
    "plt.plot([0, 100], [0, 100], '--', color='red', linewidth=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "depth = 30\n",
    "max_depth = [i for i in range(1,depth+1)] \n",
    "dtr_mae = [] \n",
    "for i in range(1,depth+1):\n",
    "    dtr = DecisionTreeRegressor(max_depth=i)\n",
    "    dtr.fit(rtx_train, rty_train)\n",
    "    dtr_y_pred = dtr.predict(rtx_test)\n",
    "    dtr_mae.append(mean_absolute_error(rty_test, dtr_y_pred))\n",
    "    \n",
    "plt.plot(max_depth, dtr_mae)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('max_depth vs MAE').figure.savefig('numcust_dtr_depth_vs_mae.png') \n",
    "plt.show()\n",
    "\n",
    "# save model to disk\n",
    "filename = 'numcust_dtr_model.sav'\n",
    "pickle.dump(regressor, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row = rtx_test.iloc[12]\n",
    "test_row = test_row.values.reshape(1, -1)\n",
    "test_row = pd.DataFrame(test_row, columns=rtx_test.columns)\n",
    "# default values to 0\n",
    "for col in test_row.columns:\n",
    "    test_row[col].values[:] = 0\n",
    "    \n",
    "test_row['Day_Monday'] = 1\n",
    "test_row['Time_Evening'] = 1\n",
    "test_row['weather_Clear'] = 1\n",
    "\n",
    "display(test_row)\n",
    "\n",
    "# make a prediction\n",
    "yhat = regressor.predict(test_row)\n",
    "print('Predicted: %.3f' % yhat[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification for Wash Item"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wash Item Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x,2), ranks)\n",
    "    return dict(zip(names, ranks))\n",
    "  \n",
    "wi_df = pd.read_csv('./dataset_w_weather&rwi&city.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wash Item Feature Selection (Boruta & RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_df = wi_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "wi_x = wi_df.drop(['Wash_Item'], axis=1)\n",
    "wi_y = wi_df['Wash_Item']\n",
    "\n",
    "wi_rf = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\", max_depth=5)\n",
    "wi_feat_selector = BorutaPy(wi_rf, n_estimators=\"auto\", random_state=1)\n",
    "wi_feat_selector.fit(wi_x.values, wi_y.values.ravel())\n",
    "\n",
    "wi_boruta_score = ranking(list(map(float, wi_feat_selector.ranking_)), wi_x.columns, order=-1)\n",
    "wi_boruta_score = pd.DataFrame(list(wi_boruta_score.items()), columns=['Features', 'Score'])\n",
    "wi_boruta_score = wi_boruta_score.sort_values(\"Score\", ascending = False)\n",
    "\n",
    "print('---------Top 10----------')\n",
    "display(wi_boruta_score.head(10))\n",
    "\n",
    "print('---------Bottom 10----------')\n",
    "display(wi_boruta_score.tail(10))\n",
    "pd.DataFrame.to_csv(wi_boruta_score.head(10), 'WashItem_Boruta_Top10.csv')\n",
    "\n",
    "sns_wi_boruta_plot = sns.catplot(x=\"Score\", y=\"Features\", data = wi_boruta_score[0:10], kind = \"bar\", height=14, aspect=1.9, palette='coolwarm')\n",
    "plt.title(\"Wash_Item Boruta Top-10 Features\")\n",
    "sns_wi_boruta_plot.figure.savefig('Wash_Item_Boruta.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_rf = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\", max_depth=5, n_estimators=100)\n",
    "wi_rf.fit(wi_x, wi_y)\n",
    "wi_rfe = RFECV(wi_rf, min_features_to_select = 1, cv = 3)\n",
    "wi_rfe.fit(wi_x, wi_y)\n",
    "\n",
    "wi_rfe_score = ranking(list(map(float, wi_rfe.ranking_)), wi_x.columns, order=-1)\n",
    "wi_rfe_score = pd.DataFrame(list(wi_rfe_score.items()), columns=['Features', 'Score'])\n",
    "wi_rfe_score = wi_rfe_score.sort_values(\"Score\", ascending = False)\n",
    "\n",
    "print('---------Top 10----------')\n",
    "display(wi_rfe_score.head(10))\n",
    "\n",
    "print('---------Bottom 10----------')\n",
    "display(wi_rfe_score.tail(10))\n",
    "pd.DataFrame.to_csv(wi_rfe_score.head(10), 'WashItem_RFE_Top10.csv')\n",
    "\n",
    "sns_wi_rfe_plot = sns.catplot(x=\"Score\", y=\"Features\", data = wi_rfe_score[0:10], kind = \"bar\", height=14, aspect=1.9, palette='coolwarm')\n",
    "plt.title(\"WashItem RFE Top-10 Features\")\n",
    "sns_wi_rfe_plot.figure.savefig('WashItem_RFE.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_X = wi_df[[\"Month\", \"Date\", \"latitude\" , \"weather\", \"rwi\", \"Kids_Category\", \"Age_Range\", \"Pants_Colour\", \"TimeSpent_minutes\"]]\n",
    "wi_y = wi_df['Wash_Item']\n",
    "\n",
    "X_wi_train, X_wi_test, y_wi_train, y_wi_test = train_test_split(wi_X, wi_y, test_size = 0.2, random_state = 10) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wi_nb = GaussianNB()\n",
    "wi_nb.fit(X_wi_train, y_wi_train)\n",
    "y_pred = wi_nb.predict(X_wi_test)\n",
    "filename = 'Wash_Item_NB.sav'\n",
    "joblib.dump(wi_nb, filename)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(wi_nb.score(X_wi_train, y_wi_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(wi_nb.score(X_wi_test, y_wi_test)))\n",
    "print('Precision Score : ' + str(precision_score(y_wi_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_wi_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_wi_test,y_pred)))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_wi_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "prob_NB = wi_nb.predict_proba(X_wi_test)\n",
    "report = classification_report(digits=6,y_true=y_wi_test, y_pred=wi_nb.predict(X_wi_test), output_dict=True)\n",
    "result = pd.DataFrame(report).transpose()\n",
    "print(result)\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh = {}\n",
    "n_class = wi_df['Wash_Item'].nunique()\n",
    "classes = wi_nb.classes_\n",
    "roc_auc_ovr = {}\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(y_wi_test, prob_NB[:,i], pos_label=i)\n",
    "    c = classes[i]\n",
    "    df_aux = X_wi_test.copy()\n",
    "    df_aux['class'] = [1 if y == c else 0 for y in y_wi_test]\n",
    "    df_aux['prob'] = prob_NB[:, i]\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "avg_roc_auc = 0\n",
    "i = 0\n",
    "with open('WASHITEM_NB_AUC.txt', 'w') as f:\n",
    "    for k in roc_auc_ovr:\n",
    "        avg_roc_auc += roc_auc_ovr[k]\n",
    "        i += 1\n",
    "        f.write(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\\n\")\n",
    "        print(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\")\n",
    "    print(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "    f.write(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "    \n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], color='red', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], color='orange', label='Class 1 vs Rest')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best').figure.savefig('WASHITEM_NB_ROC.png')\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'WASHITEM_NB.sav'\n",
    "pickle.dump(wi_nb, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_X = wi_df[[\"Month\", \"Date\", \"latitude\" , \"weather\", \"rwi\", \"Kids_Category\", \"Age_Range\", \"Pants_Colour\", \"TimeSpent_minutes\"]]\n",
    "wi_y = wi_df['Wash_Item']\n",
    "\n",
    "X_wi_train, X_wi_test, y_wi_train, y_wi_test = train_test_split(wi_X, wi_y, test_size = 0.2, random_state = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_rf = RandomForestClassifier(random_state=1, n_estimators=1000, max_depth=5)\n",
    "wi_rf.fit(X_wi_train, y_wi_train)\n",
    "y_pred = wi_rf.predict(X_wi_test)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(wi_rf.score(X_wi_train, y_wi_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(wi_rf.score(X_wi_test, y_wi_test)))\n",
    "print('Precision Score : ' + str(precision_score(y_wi_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_wi_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_wi_test,y_pred)))\n",
    "\n",
    "# confusion_matrix = confusion_matrix(y_wi_test, y_pred)\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "\n",
    "prob_RF = wi_rf.predict_proba(X_wi_test)\n",
    "\n",
    "report = classification_report(digits=6,y_true=y_wi_test,y_pred=wi_rf.predict(X_wi_test))\n",
    "print(report)\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "n_class = wi_df['Wash_Item'].nunique()\n",
    "classes = wi_rf.classes_\n",
    "roc_auc_ovr = {}\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(y_wi_test, prob_RF[:,i], pos_label=i)\n",
    "    c = classes[i]\n",
    "    df_aux = X_wi_test.copy()\n",
    "    df_aux['class'] = [1 if y == c else 0 for y in y_wi_test]\n",
    "    df_aux['prob'] = prob_RF[:, i]\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "avg_roc_auc = 0\n",
    "i = 0\n",
    "with open('WASHITEM_RF_AUC.txt', 'w') as f:\n",
    "    for k in roc_auc_ovr:\n",
    "        avg_roc_auc += roc_auc_ovr[k]\n",
    "        i += 1\n",
    "        f.write(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\\n\")\n",
    "        print(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\")\n",
    "    print(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "    f.write(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "\n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], color='red', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], color='orange', label='Class 1 vs Rest')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best').figure.savefig('WASHITEM_RF_ROC.png')\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'WASHITEM_RF.sav'\n",
    "pickle.dump(wi_rf, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_X = wi_df[[\"Month\", \"Date\", \"latitude\" , \"weather\", \"rwi\", \"Kids_Category\", \"Age_Range\", \"Pants_Colour\", \"TimeSpent_minutes\"]]\n",
    "wi_y = wi_df['Wash_Item']\n",
    "\n",
    "X_wi_train, X_wi_test, y_wi_train, y_wi_test = train_test_split(wi_X, wi_y, test_size = 0.2, random_state = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wi_KNN = KNeighborsClassifier(n_neighbors=2)\n",
    "wi_KNN.fit(X_wi_train, y_wi_train)\n",
    "y_pred = wi_KNN.predict(X_wi_test)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(wi_KNN.score(X_wi_train, y_wi_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(wi_KNN.score(X_wi_test, y_wi_test)))\n",
    "print('Precision Score : ' + str(precision_score(y_wi_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_wi_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_wi_test,y_pred)))\n",
    "\n",
    "# confusion_matrix = confusion_matrix(y_wi_test, y_pred)\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "\n",
    "with open('WASHITEM_KNN_ACC.txt', 'w') as f:\n",
    "    f.write(\"Accuracy on training set: {:.3f}\\n\".format(wi_KNN.score(X_wi_train, y_wi_train)))\n",
    "    f.write(\"Accuracy on test set: {:.3f}\\n\".format(wi_KNN.score(X_wi_test, y_wi_test)))\n",
    "    f.write('Mjority classifier Confusion Matrix\\n')\n",
    "    f.write(str(confusion_matrix))\n",
    "    \n",
    "prob_KNN = wi_KNN.predict_proba(X_wi_test)\n",
    "\n",
    "report = classification_report(digits=6,y_true=y_wi_test, y_pred=wi_KNN.predict(X_wi_test))\n",
    "print(report)\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "n_class = wi_df['Wash_Item'].nunique()\n",
    "classes = wi_KNN.classes_\n",
    "roc_auc_ovr = {}\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(y_wi_test, prob_KNN[:,i], pos_label=i)\n",
    "    c = classes[i]\n",
    "    df_aux = X_wi_test.copy()\n",
    "    df_aux['class'] = [1 if y == c else 0 for y in y_wi_test]\n",
    "    df_aux['prob'] = prob_KNN[:, i]\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "avg_roc_auc = 0\n",
    "i = 0\n",
    "with open('WASHITEM_KNN_AUC.txt', 'w') as f:\n",
    "    for k in roc_auc_ovr:\n",
    "        avg_roc_auc += roc_auc_ovr[k]\n",
    "        i += 1\n",
    "        f.write(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\\n\")\n",
    "        print(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\")\n",
    "    print(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "    f.write(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "\n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], color='red', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], color='orange', label='Class 1 vs Rest')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best').figure.savefig('WASHITEM_KNN_ROC.png')\n",
    " \n",
    "# save the model to disk\n",
    "filename = 'WASHITEM_KNN.sav'\n",
    "pickle.dump(wi_KNN, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wi_rf = RandomForestClassifier(random_state=1, n_estimators=1000, max_depth=5)\n",
    "grid_values = {'max_depth': [3, 5, 10, 20, 50]}\n",
    "grid_rf_acc = GridSearchCV(wi_rf, param_grid = grid_values, scoring = 'accuracy')\n",
    "grid_rf_acc.fit(X_wi_train, y_wi_train)\n",
    "\n",
    "# plot the results\n",
    "plt.figure()\n",
    "plt.plot(grid_rf_acc.cv_results_['param_max_depth'],\n",
    "          grid_rf_acc.cv_results_['mean_test_score'], label='accuracy')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# find best parameters\n",
    "print('Grid best parameter (max. accuracy): ', grid_rf_acc.best_params_)\n",
    "print('Grid best score (accuracy): ', grid_rf_acc.best_score_)\n",
    "\n",
    "wi_rf.fit(X_wi_train, y_wi_train)\n",
    "y_pred = wi_rf.predict(X_wi_test)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(wi_rf.score(X_wi_train, y_wi_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(wi_rf.score(X_wi_test, y_wi_test)))\n",
    "print('Precision Score : ' + str(precision_score(y_wi_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_wi_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_wi_test,y_pred)))\n",
    "\n",
    "# confusion_matrix = confusion_matrix(y_wi_test, y_pred)\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "\n",
    "prob_RF = wi_rf.predict_proba(X_wi_test)\n",
    "\n",
    "report = classification_report(digits=6,y_true=y_wi_test, y_pred=wi_rf.predict(X_wi_test))\n",
    "print(report)\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "n_class = wi_df['Wash_Item'].nunique()\n",
    "classes = wi_rf.classes_\n",
    "roc_auc_ovr = {}\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(y_wi_test, prob_RF[:,i], pos_label=i)\n",
    "    c = classes[i]\n",
    "    df_aux = X_wi_test.copy()\n",
    "    df_aux['class'] = [1 if y == c else 0 for y in y_wi_test]\n",
    "    df_aux['prob'] = prob_RF[:, i]\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "avg_roc_auc = 0\n",
    "i = 0\n",
    "with open('WASH_ITEM_RF_1_AUC.txt', 'w') as f:\n",
    "    for k in roc_auc_ovr:\n",
    "        avg_roc_auc += roc_auc_ovr[k]\n",
    "        i += 1\n",
    "        f.write(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\\n\")\n",
    "        print(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\")\n",
    "    print(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "    f.write(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "\n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], color='red', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], color='orange', label='Class 1 vs Rest')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best').figure.savefig('WASHITEM_RF_1_ROC.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_svc = SVC(probability=True, random_state=1, kernel='rbf', C=10, gamma=0.1)\n",
    "wi_svc.fit(X_wi_train, y_wi_train)\n",
    "y_pred = wi_svc.predict(X_wi_test)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(wi_svc.score(X_wi_train, y_wi_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(wi_svc.score(X_wi_test, y_wi_test)))\n",
    "print('Precision Score : ' + str(precision_score(y_wi_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_wi_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_wi_test,y_pred)))\n",
    "\n",
    "# confusion_matrix = confusion_matrix(y_wi_test, y_pred)\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "\n",
    "prob_svc = wi_svc.predict_proba(X_wi_test)\n",
    "report = classification_report(digits=6,y_true=y_wi_test, y_pred=wi_svc.predict(X_wi_test))\n",
    "print(report)\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "n_class = wi_df['Wash_Item'].nunique()\n",
    "classes = wi_svc.classes_\n",
    "roc_auc_ovr = {}\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(y_wi_test, prob_svc[:,i], pos_label=i)\n",
    "    c = classes[i]\n",
    "    df_aux = X_wi_test.copy()\n",
    "    df_aux['class'] = [1 if y == c else 0 for y in y_wi_test]\n",
    "    df_aux['prob'] = prob_svc[:, i]\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "avg_roc_auc = 0\n",
    "i = 0\n",
    "with open('WASH_ITEM_SVC_1_AUC.txt', 'w') as f:\n",
    "    for k in roc_auc_ovr:\n",
    "        avg_roc_auc += roc_auc_ovr[k]\n",
    "        i += 1\n",
    "        f.write(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\\n\")\n",
    "        print(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\")\n",
    "    print(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "    f.write(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "\n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], color='red', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], color='orange', label='Class 1 vs Rest')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best').figure.savefig('WASHITEM_SVC_1_ROC.png')\n",
    "\n",
    "# save the model to disk \n",
    "filename = 'WASH_ITEM_SVC_1.sav'\n",
    "pickle.dump(wi_svc, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = KNeighborsClassifier(n_neighbors=3)\n",
    "estimators.append(('knn', model2))\n",
    "model3 = RandomForestClassifier(random_state=1, n_estimators=1000, max_depth=10)\n",
    "\n",
    "estimators.append(('rf', model3))\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "ensemble.fit(X_wi_train, y_wi_train)\n",
    "y_pred = ensemble.predict(X_wi_test)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(ensemble.score(X_wi_train, y_wi_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(ensemble.score(X_wi_test, y_wi_test)))\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_wi_test,y_pred)))\n",
    "print('Precision Score : ' + str(precision_score(y_wi_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_wi_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_wi_test,y_pred)))\n",
    "\n",
    "prob_ESB = ensemble.predict_proba(X_wi_test)\n",
    "\n",
    "sk_report = classification_report(digits=6,y_true=y_wi_test, y_pred=ensemble.predict(X_wi_test))\n",
    "print(sk_report)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'WASH_ITEM_ESB_VOT.sav'\n",
    "pickle.dump(ensemble, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression()))\n",
    "level0.append(('knn', KNeighborsClassifier()))\n",
    "level0.append(('cart', DecisionTreeClassifier()))\n",
    "level0.append(('svm', SVC()))\n",
    "level0.append(('bayes', GaussianNB()))\n",
    "level1 = LogisticRegression()\n",
    "stacked_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "\n",
    "models = dict()\n",
    "models['lr'] = LogisticRegression()\n",
    "models['knn'] = KNeighborsClassifier()\n",
    "models['cart'] = DecisionTreeClassifier()\n",
    "models['svm'] = SVC()\n",
    "models['bayes'] = GaussianNB()\n",
    "models['stacking'] = stacked_model\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X_wi_test, y_wi_test, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "results, names = list(), list() \n",
    "for name, model in models.items():\n",
    "  scores = evaluate_model(model, X_wi_train, y_wi_train)\n",
    "  results.append(scores)\n",
    "  names.append(name)\n",
    "  print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'WASH_ITEM_ESB_STACKING.sav'\n",
    "pickle.dump(stacked_model, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=0)\n",
    "XSMOTE_wi_train, XSMOTE_wi_test, ySMOTE_wi_train, ySMOTE_wi_test = train_test_split(wi_X, wi_y.values.ravel(), test_size=0.4, random_state=10)\n",
    "columns = XSMOTE_wi_train.columns\n",
    "sm_data_X, sm_data_y = sm.fit_resample(XSMOTE_wi_train, ySMOTE_wi_train)\n",
    "os_data_X = pd.DataFrame(data=sm_data_X, columns=columns)\n",
    "os_data_y = pd.DataFrame(data=sm_data_y, columns=['Wash_Item'])\n",
    "\n",
    "\n",
    "print(\"Oversampled  data has {} rows and {} columns\".format(len(os_data_X),len(os_data_X.columns)))\n",
    "print(\"Proportion of 1 data in oversampled data is \",len(os_data_y[os_data_y['Wash_Item']==0])/len(os_data_X))\n",
    "print(\"Proportion of 2 data in oversampled data is \",len(os_data_y[os_data_y['Wash_Item']==1])/len(os_data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XSMOTE_wi_train, XSMOTE_wi_test, ySMOTE_wi_train, ySMOTE_wi_test = train_test_split(os_data_X, os_data_y, test_size=0.4, random_state=10)\n",
    "ySMOTE_wi_test = ySMOTE_wi_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_Smote_RF = RandomForestClassifier(random_state=1, n_estimators=1000, max_depth=100)\n",
    "wi_Smote_RF.fit(XSMOTE_wi_train, ySMOTE_wi_train)\n",
    "ySMOTE_pred = wi_Smote_RF.predict(XSMOTE_wi_test)\n",
    "\n",
    "# Calculate the overall accuracy on test set \n",
    "print(\"Accuracy on training set: {:.3f}\".format(ensemble.score(X_wi_train, y_wi_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(ensemble.score(X_wi_test, y_wi_test)))\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_wi_test,y_pred)))\n",
    "print('Precision Score : ' + str(precision_score(y_wi_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_wi_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_wi_test,y_pred)))\n",
    "\n",
    "# confusion_matrix = confusion_matrix(ySMOTE_wi_test, ySMOTE_pred)\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "\n",
    "prob_RF = wi_Smote_RF.predict_proba(XSMOTE_wi_test)\n",
    "\n",
    "report = classification_report(digits=6,y_true=ySMOTE_wi_test, y_pred=wi_Smote_RF.predict(XSMOTE_wi_test))\n",
    "print(report)\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "n_class = wi_df['Wash_Item'].nunique()\n",
    "classes = wi_Smote_RF.classes_\n",
    "roc_auc_ovr = {}\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(ySMOTE_wi_test, prob_RF[:,i], pos_label=i)\n",
    "    c = classes[i]\n",
    "    df_aux = XSMOTE_wi_test.copy()\n",
    "    df_aux['class'] = [1 if y == c else 0 for y in ySMOTE_wi_test]\n",
    "    df_aux['prob'] = prob_RF[:, i]\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "avg_roc_auc = 0\n",
    "i = 0\n",
    "with open('SMOTE_WASH_ITEM_RF_1_AUC.txt', 'w') as f:\n",
    "    for k in roc_auc_ovr:\n",
    "        avg_roc_auc += roc_auc_ovr[k]\n",
    "        i += 1\n",
    "        f.write(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\\n\")\n",
    "        print(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\")\n",
    "    print(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "    f.write(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "\n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], color='red', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], color='orange', label='Class 1 vs Rest')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='-')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best').figure.savefig('SMOTE_WASHITEM_RF_1_ROC.png')\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'SMOTE_WASH_ITEM_RF_1.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_nb_Smote = GaussianNB()\n",
    "wi_nb_Smote.fit(XSMOTE_wi_train, ySMOTE_wi_train)\n",
    "ySMOTE_pred = wi_nb_Smote.predict(XSMOTE_wi_test)\n",
    "filename = 'SMOTE_Wash_Item_NB.sav'\n",
    "joblib.dump(wi_nb_Smote, filename)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(wi_nb_Smote.score(XSMOTE_wi_train, ySMOTE_wi_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(wi_nb_Smote.score(XSMOTE_wi_test, ySMOTE_wi_test)))\n",
    "print('Precision Score : ' + str(precision_score(ySMOTE_wi_test,ySMOTE_pred)))\n",
    "print('Recall Score : ' + str(recall_score(ySMOTE_wi_test,ySMOTE_pred)))\n",
    "print('F1 Score : ' + str(f1_score(ySMOTE_wi_test,ySMOTE_pred)))\n",
    "\n",
    "# confusion_matrix = confusion_matrix(ySMOTE_wi_test, ySMOTE_pred)\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "\n",
    "prob_NB = wi_nb_Smote.predict_proba(XSMOTE_wi_test)\n",
    "report = classification_report(digits=6,y_true=ySMOTE_wi_test, y_pred=wi_nb_Smote.predict(XSMOTE_wi_test), output_dict=True)\n",
    "result = pd.DataFrame(report).transpose()\n",
    "print(result)\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh = {}\n",
    "n_class = wi_df['Wash_Item'].nunique()\n",
    "classes = wi_nb_Smote.classes_\n",
    "roc_auc_ovr = {}\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(ySMOTE_wi_test, prob_NB[:,i], pos_label=i)\n",
    "    c = classes[i]\n",
    "    df_aux = XSMOTE_wi_test.copy()\n",
    "    df_aux['class'] = [1 if y == c else 0 for y in ySMOTE_wi_test]\n",
    "    df_aux['prob'] = prob_NB[:, i]\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "avg_roc_auc = 0\n",
    "i = 0\n",
    "with open('SMOTE_WASHITEM_NB_AUC.txt', 'w') as f:\n",
    "    for k in roc_auc_ovr:\n",
    "        avg_roc_auc += roc_auc_ovr[k]\n",
    "        i += 1\n",
    "        f.write(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\\n\")\n",
    "        print(f\"Class {k} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\")\n",
    "    print(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "    f.write(f\"Average ROC AUC OvR: {avg_roc_auc/i:.4f}\")\n",
    "    \n",
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], color='red', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], color='orange', label='Class 1 vs Rest')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best').figure.savefig('SMOTE_WASHITEM_NB_ROC.png')\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'SMOTE_WASH_ITEM_NB.sav'\n",
    "pickle.dump(wi_nb_Smote, open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate = plt.scatter(imp_df['latitude'],imp_df['longitude'])\n",
    "plt.xlabel('city')\n",
    "plt.ylabel('rwi')\n",
    "coordinate.figure.savefig('lat_lng_scatter.png')\n",
    "coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imp_df[['longitude', 'latitude']]\n",
    "sil = []\n",
    "kmax = 10\n",
    "\n",
    "for k in range(2, kmax+1):\n",
    "  kmeans = KMeans(n_clusters = k).fit(X)\n",
    "  labels = kmeans.labels_\n",
    "  sil.append(silhouette_score(X, labels, metric = 'euclidean'))\n",
    "  \n",
    "plt.plot(range(2, kmax+1), sil, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette Method For Optimal k')\n",
    "plt.show()\n",
    "plt.savefig('long_lat_silhouette.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters = 3, verbose=0)\n",
    "model.fit(X)\n",
    "clusters = model.fit_predict(X)\n",
    "\n",
    "df_clusters = imp_df.copy()\n",
    "df_clusters['Clusters'] = clusters \n",
    "plt.scatter(df_clusters['longitude'],df_clusters['latitude'],c=df_clusters['Clusters'],cmap='autumn')\n",
    "plt.savefig('long_lat_clustered.png')\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'long_lat_cluster.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(imp_df[['Age_Range']])\n",
    "imp_df['Age_Range'] = scaler.transform(imp_df[['Age_Range']])\n",
    "imp_df.head(5)\n",
    "\n",
    "coordinate = plt.scatter(imp_df['rwi'],imp_df['Age_Range'])\n",
    "plt.xlabel('city')\n",
    "plt.ylabel('rwi')\n",
    "coordinate.figure.savefig('city_rwi_scatter.png')\n",
    "coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil = []\n",
    "kmax = 10\n",
    "for k in range(2, kmax+1):\n",
    "  kmeans = KMeans(n_clusters = k).fit(X)\n",
    "  labels = kmeans.labels_\n",
    "  sil.append(silhouette_score(X, labels, metric = 'euclidean'))\n",
    "  \n",
    "plt.plot(range(2, kmax+1), sil, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette Method For Optimal k')\n",
    "plt.show()\n",
    "plt.savefig('age_range_rwi_silhouette.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imp_df[['Age_Range', 'rwi']]\n",
    "\n",
    "model = KMeans(n_clusters = 3, verbose=0) \n",
    "model.fit(X)\n",
    "clusters = model.fit_predict(X)\n",
    "\n",
    "df_clusters = imp_df.copy()\n",
    "df_clusters['Clusters'] = clusters \n",
    "plt.scatter(df_clusters['Age_Range'],df_clusters['rwi'],c=df_clusters['Clusters'],cmap='autumn')\n",
    "plt.savefig('age_range_rwi_clustered.png')\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'age_range_rwi_cluster.sav' \n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f50f518e30a1733e8e8c8b608de131219c9974acb0d02ce13c65eb2a815a608c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
